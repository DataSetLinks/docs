<img height="100" src="/logo/dsl_logo_transparent.png" />

# Welcome to DataLinks!

Click on [My Data](https://datalinks.tech/home/data) upload or find your datasets through the UI. Below you'll find some docs on how to interact with DataLinks via the API!

## Querying data

You can send expressive queries to the backend that can filter, do search arounds, aggregations and more!

```
 1 curl -H "Authorization: Bearer token1234abcdefg"
 2      -X POST https://api.datasetlinks.com/api/data/username/namespace/objectname
 3      --data 'OntologyObject("objectname").get()'
```

Line by line:

1 - You need to provide us with a Bearer token for your user. We don't have a self service token generator yet, but you can ask the DataLinks team to give you one by reaching out to us.


2 - These three properties **username**/**namespace**/**objectname** can be found in the dataset url path by clicking My Data on the left menu and navigating to the dataset you want to query.


3 - The query you want to run. The reason why this curl is a Post is so you can send the query on the body. Our query language keeps growing and soon we'll publish documentation for the whole feature space. Till then below are some examples:

```
  1 Getting all data - OntologyObject("objectname").get()
  2 Filtering a column - OntologyObject("objectname").filter("columnName==Foo").get()
  3 Filter with Logic expressions - OntologyObject("objectname").filter("columnName==Foo 
    && column2 > 2").get()
  4 Following links - OntologyObject("objectname").searchAround().find("otherObject").get()
```

## Creating a dataset

The api endpoint below will create a dataset under your user. You always need to create a dataset before you upload data to it.

```
1  curl -H "Authorization: Bearer token1234abcdefg"
2       -X POST https://api.datasetlinks.com/api/ingest/new/namespace/objectname
```

Line by line:

1 - You need to provide us with a Bearer token for your user. We don't have a self service token generator yet, but you can ask the DataLinks team to give you one by reaching out to us.


2 - The **namespace** is the name you want to group your datasets around. An **objectname** with the same name may exist in different namespaces, for example *companies* exists in both *sanctions* and in the *company registry*.

## Uploading data into a dataset

<Warning>
  Before uploading data to a dataset you first need to create it! Use the instructions above or the UI in [My Data](https://datalinks.tech/home/data)
</Warning>

### Uploading structured data

Uploading data can currently be done by uploading a structured json. You will need write permitions to that dataset, which you will have if it's a dataset you created. If the dataset belongs to someone else you'll need to request write permissions to be enabled for you in that dataset. This is currently supported but there isn't any UI element to do it, until there is if you ask the DataLinks team we'll enable the sharing on the backend.

```
  curl -H "Authorization: Bearer token1234abcdefg"
       -X POST https://api.datasetlinks.com/api/ingest/namespace/objectname
       --data '{"data": [{"col1": 123, "col2": "foo"}, {"col1": 444, "col2": "bar"}] }'
```

Line by line:

1 - You need to provide us with a Bearer token for your user. We don't have a self service token generator yet, but you can ask the DataLinks team to give you one by reaching out to us.


2 - The **namespace** is the name you want to group your datasets around. An **objectname** with the same name may exist in different namespaces, for example *companies* exists in both *sanctions* and in the *company registry*.


3 - In the body of your **POST** you need to pass string with a JSON containing an array of rows within the property **data**. The example above sends two rows with the same columns.

## Infering new columns when uploading data

When uploading data you can infer columns from existing ones. This uses our AI for auto derivations, you'll need to explain what the columns that you want are, and which steps you want to execute. We currently support three steps, Table, Rows and Normalize.

```
1  curl -H "Authorization: Bearer token1234abcdefg"
2       -X POST https://api.datasetlinks.com/api/ingest/namespace/objectname
3       --data '{
4                 "data": [{"col1": 123, "col2": "foo", "colWithATable":                5                 "direction,size;up,large"}, {"col1": 444, "col2": "bar"}]
6                   "infer": {
7                     "steps": [{
8                        "type": "table"
9                        "deriveFrom": "colWithATable"
10                       "helperPrompt": "This text contains a table comma separated,  
11                       but instead of line breaks we are using a &apos;;&apos;."
12                    }]
13                  }
14               }'
```

Line by line:

1 - The inference can have multiple steps which are combined by our AI. The example above extracts a simple table, but you can have other steps suce has **table**, **rows**, **normalize**.


2 - In this specific example we specify we want a **table** extracted from a specific column. This will extract all columns available from the text provided. You can use the **helperPrompt** to guide the AI system to have higher accuracy during extraction.

### Table extraction

The table step will try to extract a table from any piece of text provided. This can be free text like a review of a restaurant, a report from a flight log or a finantial instrument notice. If this step is used alone, it will likely create somewhat random columns that will need to be normalized later. The table step will create new columns and append these to the rows in the existing table.

<Tip>
  Pro tip: Calling an extraction with just the **table** step is a great way to see what kind of **structured** data can be generated from your unstructured data.
</Tip>

```
1  {
2    "type": "table"
3    "deriveFrom": "columnName from where the column should be derived from"
4    "helperPrompt": "This text contains a table comma separated, but instead of line breaks 5    we are using a &apos;;&apos;."
6  }
```

#### Rows

Whenever you have a table in json format within a column you can transform it directly into a table. The json should be an array of objects, each key in the object will be mapped to a new column.

```
  {
    "type": "rows"
    "deriveFrom": "columnName from where the json content will be read"
  }
```

#### Normalize names

Use the Normalize names step to consolidate the schema or column space. Here you should pass the columns that you want outputed in your table that gets indexed into datalinks.

```
  {
    "type": "normalize"
    "targetCols": {
        "NameOfTheColumn": "Description for what this column is, which will enable the AI to automatically transform other names into
        the desired schema, you can have as many columns as you want"
    },
    "helperPrompt": "This prompt will help the llm understand what the domain you are operating under is. So that can correctly
        interpret the existing column names and the target columns."
  }
```