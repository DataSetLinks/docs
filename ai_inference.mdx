# AI-Powered Data Inference

DataLinks provides powerful AI capabilities to automatically derive new columns, extract structured data from unstructured content, and normalize your data during the ingestion process.

## Overview

When uploading data to DataLinks, you can leverage our AI inference engine to:

- Extract structured tables from text descriptions
- Transform JSON data into tabular format
- Normalize column names for consistency
- Combine multiple transformation steps in a single request

This guide explains how to use these AI features to enhance your data.

## Inference Steps

DataLinks supports three types of inference steps:

1. **Table**: Extract structured data from unstructured text
2. **Rows**: Transform JSON arrays into tabular data
3. **Normalize**: Standardize column names and formats

You can combine these steps in a single request to create powerful data transformation pipelines.

## Basic Usage

To use AI inference, include an `infer` object in your data upload request:

```bash
curl -H "Authorization: Bearer YOUR_API_TOKEN" \
     -X POST https://api.datalinks.com/api/v1/ingest/my-namespace/my-dataset \
     --data '{
       "data": [
         {
           "col1": 123,
           "col2": "foo",
           "colWithATable": "direction,size;up,large;down,small"
         },
         {
           "col1": 444,
           "col2": "bar"
         }
       ],
       "infer": {
         "steps": [
           {
             "type": "table",
             "deriveFrom": "colWithATable",
             "helperPrompt": "This text contains a table comma separated, but instead of line breaks we are using a semicolon."
           }
         ]
       }
     }'
```

This example extracts a table from the `colWithATable` column, creating new columns based on the structured data found in the text.

## Table Extraction

### Description

The Table step extracts structured data from unstructured text. It can identify tables in various formats, including:

- Comma-separated values
- Semicolon-separated records
- Free-form text descriptions
- Reports and documents

### Configuration

```json
{
  "type": "table",
  "deriveFrom": "sourceColumnName",
  "helperPrompt": "Optional guidance for the AI about the data format"
}
```

### Parameters

| Parameter | Description |
|-----------|-------------|
| `type` | Must be set to `"table"` |
| `deriveFrom` | The name of the column containing the text to analyze |
| `helperPrompt` | Optional guidance to help the AI understand the data format |

<Tip>
  **Pro tip:** Calling an extraction with just the **table** step is a great way to see what kind of structured data can be generated from your unstructured data.
</Tip>

### Example

If your data contains a column with text like:

```
product,price,quantity;laptop,999,5;smartphone,499,10
```

The Table step can extract this into structured columns:

```json
{
  "product": "laptop",
  "price": 999,
  "quantity": 5
}
```

## Rows Transformation

### Description

The Rows step transforms JSON arrays stored as strings in a column into structured tabular data. Each object in the array becomes a row, with object keys becoming column names.

### Configuration

```json
{
  "type": "rows",
  "deriveFrom": "sourceColumnName"
}
```

### Parameters

| Parameter | Description |
|-----------|-------------|
| `type` | Must be set to `"rows"` |
| `deriveFrom` | The name of the column containing the JSON array |

### Example

If your data contains a column with a JSON string like:

```json
[
  {"name": "John", "age": 30},
  {"name": "Alice", "age": 25}
]
```

The Rows step will transform this into:

```json
{
  "name": "John",
  "age": 30
}
```

and

```json
{
  "name": "Alice",
  "age": 25
}
```

## Normalize Names

### Description

The Normalize step standardizes column names across your data. This is particularly useful after a Table extraction step, which might produce inconsistent column names.

### Configuration

```json
{
  "type": "normalize",
  "targetCols": {
    "StandardColumnName": "Description of what this column represents",
    "AnotherColumn": "Description of another column"
  },
  "helperPrompt": "Optional context about the domain to help the AI understand the data"
}
```

### Parameters

| Parameter | Description |
|-----------|-------------|
| `type` | Must be set to `"normalize"` |
| `targetCols` | Object mapping desired column names to their descriptions |
| `helperPrompt` | Optional context about the domain to help the AI understand the data |

### Example

If your data has inconsistent column names like "First Name", "fname", and "given_name", you can normalize them:

```json
{
  "type": "normalize",
  "targetCols": {
    "firstName": "The person's first or given name"
  },
  "helperPrompt": "This is customer data with various name formats"
}
```

## Combining Steps

You can combine multiple inference steps to create powerful transformation pipelines:

```json
{
  "infer": {
    "steps": [
      {
        "type": "table",
        "deriveFrom": "textDescription"
      },
      {
        "type": "normalize",
        "targetCols": {
          "product": "Product name",
          "price": "Product price in USD",
          "quantity": "Available quantity"
        }
      }
    ]
  }
}
```

This example first extracts a table from the "textDescription" column, then normalizes the resulting columns.

## Best Practices

1. **Start Simple**: Begin with a single inference step and add complexity as needed
2. **Use Helper Prompts**: Provide context to improve AI accuracy
3. **Normalize After Extraction**: Always normalize columns after table extraction
4. **Test with Sample Data**: Try inference on a small dataset first
5. **Review Results**: Check the output to ensure the AI correctly interpreted your data

## Limitations

- Very large text fields may be truncated
- Complex nested structures might not be fully extracted
- Highly domain-specific terminology may require detailed helper prompts

## Next Steps

After using AI inference to enhance your data:
- [Query your enriched data](Querying_data) using our Semantic Layer
- [Upload more data](Uploading_data_into_a_dataset) to expand your dataset
- Explore connections between your enhanced data and other datasets