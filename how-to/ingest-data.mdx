---
title: "How to Ingest Data"
description: "Load data into a DataLinks dataset using the web platform or API."
---

Once you've created a dataset, the next step is to ingest data into it. DataLinks accepts data in multiple formats and can automatically clean, normalize, and validate your data during ingestion. This guide covers how to ingest data through the web platform for quick uploads and through the API for automation and integration.

If you're new to DataLinks, you may want to start with the explanatory article [Ingestion and Cleaning](/concepts/ingest-clean) to learn how DataLinks transforms raw data into structured, queryable information.

## Ingest data using the API

This section shows how to ingest data into an existing DataLinks dataset by calling the REST API directly from Python. You'll ingest a list of employee records into a dataset named `employee_records` inside the `hr_demo` namespace.

#### Prerequisites

Before you begin, make sure you have:

- A DataLinks account
- A valid bearer token for the DataLinks API
- An existing dataset to ingest data into (see [How to Create a Dataset](/how-to/create-dataset))
- A Python environment set up for running scripts
- The `requests` library installed

### **Step 1: Install dependencies**

```
pip install requests
```

or

```
pip3 install requests
```

**Step 2: Configure environment variables**

Set the following environment variables so sensitive values are not hardcoded in your script:

<Tabs>
  <Tab title="Windows">
    For the current session:

    ```powershell
    $env:DATALINKS_TOKEN="YOUR_BEARER_TOKEN"
    $env:DATALINKS_NAMESPACE="hr_demo"
    $env:DATALINKS_DATASET="employee_records"
    ```

    Persist across future sessions:

    ```powershell
    setx DATALINKS_TOKEN "YOUR_BEARER_TOKEN"
    setx DATALINKS_NAMESPACE "hr_demo"
    setx DATALINKS_DATASET "employee_records"
    ```

    After using `setx`, open a new PowerShell window before running your script.
  </Tab>
  <Tab title="Linux">
    ```bash
    export DATALINKS_TOKEN="YOUR_BEARER_TOKEN"
    export DATALINKS_NAMESPACE="hr_demo"
    export DATALINKS_DATASET="employee_records"
    ```

    These apply to the current shell session. Add them to `~/.bashrc` or `~/.zshrc` if you want them to persist automatically.
  </Tab>
  <Tab title="MacOS">
    

    ```bash
    export DATALINKS_TOKEN="YOUR_BEARER_TOKEN"
    export DATALINKS_NAMESPACE="hr_demo"
    export DATALINKS_DATASET="employee_records"
    ```

    These apply to the current terminal session. For persistence, place them in `~/.zshrc`.
  </Tab>
</Tabs>

### **Step 2: Create the ingestion script (Python)**

Create a Python file, for example `ingest_data.py`, and add the following code:

```python
import os
import json
import requests

BASE_URL = "https://api.datalinks.com/api/v1"

token = os.environ["DATALINKS_TOKEN"]
namespace = os.environ["DATALINKS_NAMESPACE"]
dataset_name = os.environ["DATALINKS_DATASET"]

url = f"{BASE_URL}/ingest/{namespace}/{dataset_name}"

headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json",
}

# Sample data to ingest
data = [
    {"id": "001", "name": "Alice Johnson", "age": "32", "department": "Engineering", "salary": "95000"},
    {"id": "002", "name": "Bob Smith", "age": "45", "department": "Marketing", "salary": "78000"},
    {"id": "003", "name": "Carol Davis", "age": "28", "department": "Engineering", "salary": "87000"},
]

payload = {
    "data": data,
}

response = requests.post(
    url,
    headers=headers,
    data=json.dumps(payload),
    timeout=60,
)

print("Status:", response.status_code)

if response.headers.get("content-type", "").startswith("application/json"):
    print(json.dumps(response.json(), indent=2))
else:
    print(response.text)

response.raise_for_status()
print("\nData ingested successfully.")
```

### **Step 3: Run the script**

Run the script using your normal Python workflow:

```bash
python ingest_data.py
```

### **How this works**

This request:

- Calls `POST /ingest/{namespace}/{datasetName}` to load data into an existing dataset
- Authenticates using a bearer token in the `Authorization` header
- Sends data as a JSON array of objects, where each object represents one record
- Returns information about successful and failed records

The `data` field accepts an array of dictionaries. Each dictionary represents a row, with keys as column names and values as the data for that column.

### Common issues

**Authentication errors** If the request fails with a 401 error, confirm that:

- The token environment variable is set correctly
- The `Authorization` header uses the `Bearer` format

**Dataset not found** If you see a 404 error, check that:

- The namespace and dataset name in the URL match an existing dataset
- You have permission to access the dataset

**Data format errors** If you see a 400 error, verify that:

- The `data` field contains a valid JSON array
- Each record in the array is a dictionary with string keys

## Ingest data in the web platform

If you prefer a visual interface, you can ingest data directly in the DataLinks web platform. This approach is ideal for quick uploads or when you want to preview how DataLinks will process your data.

To ingest data in the web platform, follow these steps:

1. **Log in** to your DataLinks account.
2. Navigate to the dataset where you want to ingest data.
3. Click anywhere in the **Click to upload** area to open the file explorer.
4. Select your data file (JSON, CSV, or TXT), then click **Open**.
5. Wait while DataLinks ingests your data.
6. Click the **eye icon** under Ingestion Results to review the data and confirm that everything looks correct.
7. Scroll down to the bottom of the page, then click **Upload data**.
8. Click **Append** to confirm.

<Check>
  Your data will be added to the dataset and will be ready for linking and querying.
</Check>

For unstructured data, enter helper prompts and field definitions before uploading. Files attached before entering prompts will not have transformations applied.

## Working with inference steps

DataLinks can automatically transform and clean your data during ingestion using inference steps. These steps run in sequence to extract structure, normalize schemas, and validate data quality.

#### Table extraction

Use the `table` step to extract structured data from text containing tables:

```json
{
  "type": "table",
  "deriveFrom": "raw_column",
  "helperPrompt": "This text contains a comma-separated table with semicolons as line breaks."
}
```

#### Normalize

Use the `normalize` step to consolidate column names into a consistent schema:

```json
{
  "type": "normalize",
  "mode": "all-in-one",
  "targetCols": {
    "employee_name": "Full name of the employee",
    "department": "Work department name",
    "salary": "Annual salary in USD"
  },
  "helperPrompt": "This data contains HR employee records."
}
```

The normalize step supports three modes:

- **all-in-one:** Uses a single prompt to normalize all columns at once
- **field-by-field:** Normalizes each field individually for complex schemas
- **embeddings:** Uses embeddings to match column names for large datasets

#### Validate

Use the `validate` step to verify data quality:

```json
{
  "type": "validate",
  "mode": "fields",
  "columns": ["email", "phone"]
}
```

Validation modes:

- **rows:** Validates entire rows against expected patterns
- **fields:** Validates specific columns
- **regex:** Validates using regular expressions

#### ReverseGeo

Use the `reverseGeo` step to extract location data from coordinates:

```json
{
  "type": "reverseGeo",
  "deriveFrom": "coordinates"
}
```

#### Using inference steps with the API

To apply inference steps when ingesting via the API, include an `infer` object in your request payload:

```python
import os
import json
import requests

BASE_URL = "https://api.datalinks.com/api/v1"

token = os.environ["DATALINKS_TOKEN"]
namespace = os.environ["DATALINKS_NAMESPACE"]
dataset_name = os.environ["DATALINKS_DATASET"]

url = f"{BASE_URL}/ingest/{namespace}/{dataset_name}"

headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json",
}

# Data with unstructured text to transform
data = [
    {"raw_text": "Alice Johnson, Engineering, $95,000/year"},
    {"raw_text": "Bob Smith, Marketing, $78,000/year"},
]

payload = {
    "data": data,
    "infer": {
        "steps": [
            {
                "type": "table",
                "deriveFrom": "raw_text",
                "helperPrompt": "Extract employee name, department, and salary from each line."
            },
            {
                "type": "normalize",
                "mode": "all-in-one",
                "targetCols": {
                    "employee_name": "Full name of the employee",
                    "department": "Work department",
                    "annual_salary": "Yearly compensation in USD"
                },
                "helperPrompt": "This data contains HR employee records."
            }
        ]
    }
}

response = requests.post(
    url,
    headers=headers,
    data=json.dumps(payload),
    timeout=120,
)

print("Status:", response.status_code)
print(json.dumps(response.json(), indent=2))
```

## Uploading large files

For files larger than 5MB, use the multipart upload flow to improve reliability. This approach splits the file into parts and uploads them separately.

### Step 1: Prepare the upload

Call the prepare endpoint to get presigned URLs for each part:

```python
import os
import json
import requests

BASE_URL = "https://api.datalinks.com/api/v1"
token = os.environ["DATALINKS_TOKEN"]
username = "your_username"
namespace = "hr_demo"
dataset_name = "employee_records"

# Step 1: Prepare the multipart upload
prepare_url = f"{BASE_URL}/multipart/prepare/{username}/{namespace}/{dataset_name}"

headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json",
}

file_size = os.path.getsize("large_data.json")

prepare_payload = {
    "filename": "large_data.json",
    "size": file_size,
}

response = requests.post(prepare_url, headers=headers, json=prepare_payload)
upload_info = response.json()

upload_id = upload_info["uploadId"]
presigned_urls = upload_info["presignedUrls"]
s3_key = upload_info["key"]
```

### Step 2: Upload each part

```python
# Step 2: Upload each part
parts = []
part_size = 5 * 1024 * 1024  # 5MB per part

with open("large_data.json", "rb") as f:
    for i, url in enumerate(presigned_urls):
        part_data = f.read(part_size)
        part_response = requests.put(url, data=part_data)
        
        etag = part_response.headers["ETag"]
        parts.append({
            "partNumber": i + 1,
            "etag": etag.strip('"')
        })
```

### Step 3: Complete the upload

```python
# Step 3: Finish the multipart upload
finish_url = f"{BASE_URL}/multipart/finish/{username}/{namespace}/{dataset_name}"

finish_payload = {
    "uploadId": upload_id,
    "key": s3_key,
    "parts": parts,
}

response = requests.post(finish_url, headers=headers, json=finish_payload)
print("Upload complete:", response.json())
```

### Aborting an upload

If an upload fails or needs to be canceled, use the abort endpoint:

```python
abort_url = f"{BASE_URL}/multipart/abort/{username}/{namespace}/{dataset_name}"

abort_payload = {
    "uploadId": upload_id,
    "key": s3_key,
}

response = requests.post(abort_url, headers=headers, json=abort_payload)
```

## Next steps

Once your data is ingested, you can:

- **Review and manage connections** between datasets. See [How to Manage Connections](/how-to/manage-connections).
- **Query your data** using natural language or structured queries. See [How to Query Your Data](/how-to/query-data).
- **Rebuild links** if you have ingested new data that should be connected to existing datasets.