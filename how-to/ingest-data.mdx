---
title: "How to ingest data"
description: "Load data into a DataLinks dataset using the web platform or API."
sidebarTitle: "Ingest data"
---

Once you've created a dataset, the next step is to ingest data into it. DataLinks accepts data in multiple formats and can automatically clean, normalize, and validate your data during ingestion. This guide covers how to ingest data through the web platform for quick uploads and through the API for automation and integration.

If you're new to DataLinks, you may want to start with the explanatory article [Ingestion and Cleaning](/concepts/ingest-clean) to learn how DataLinks transforms raw data into structured, queryable information.

## Ingest data using the API

This section shows how to ingest data into an existing DataLinks dataset by calling the REST API directly from Python. You'll ingest a list of employee records into a dataset named `employee_records` inside the `hr_demo` namespace.

## Sample data

Download the sample CSV file to follow along: employee_records_sample.csv

This file contains 15 employee records with intentional inconsistencies that mirror real-world data problems:

| Issue                    | Examples in the data                                |
| ------------------------ | --------------------------------------------------- |
| Inconsistent casing      | "Engineering", "ENGINEERING", "engineering"         |
| Department abbreviations | "HR" vs "Human Resources"                           |
| Salary format variations | "\$95,000/year", "78000", "87000 annually"          |
| Date format chaos        | "2022-03-15", "03/22/2021", "Jan 5 2020"            |
| Phone number formats     | "(555) 555-4567", "555.555.5678", "+1-555-555-6789" |

You'll see how DataLinks handles these inconsistencies in the Cleaning data with inference steps section.

#### Prerequisites

Before you begin, make sure you have:

- A DataLinks account
- A valid API token (see [How to Get an API Token](/how-to/get-api-token))
- An existing dataset named `employee_records` inside a `hr_demo` namespace (see [How to Create a Dataset](/how-to/create-dataset))
- Python 3.x with the `requests` library installed
- A copy of the sample data

### Step 1: Install dependencies

```
pip install requests
```

or

```
pip3 install requests
```

### Step 2: Configure environment variables

Set the following environment variables so sensitive values are not hardcoded in your script:

<Tabs>
  <Tab title="Windows">
    For the current session:

    ```powershell
    $env:DATALINKS_TOKEN="YOUR_BEARER_TOKEN"
    $env:DATALINKS_NAMESPACE="hr_demo"
    $env:DATALINKS_DATASET="employee_records"
    ```

    Persist across future sessions:

    ```powershell
    setx DATALINKS_TOKEN "YOUR_BEARER_TOKEN"
    setx DATALINKS_NAMESPACE "hr_demo"
    setx DATALINKS_DATASET "employee_records"
    ```

    After using `setx`, open a new PowerShell window before running your script.
  </Tab>
  <Tab title="Linux">
    ```bash
    export DATALINKS_TOKEN="YOUR_BEARER_TOKEN"
    export DATALINKS_NAMESPACE="hr_demo"
    export DATALINKS_DATASET="employee_records"
    ```

    <Note>
      These apply to the current shell session. Add them to `~/.bashrc` or `~/.zshrc` if you want them to persist automatically.
    </Note>
  </Tab>
  <Tab title="MacOS">
    ```bash
    export DATALINKS_TOKEN="YOUR_BEARER_TOKEN"
    export DATALINKS_NAMESPACE="hr_demo"
    export DATALINKS_DATASET="employee_records"
    ```

    <Note>
      These apply to the current terminal session. For persistence, place them in `~/.zshrc`.
    </Note>
  </Tab>
</Tabs>

### Step 3: Create the ingestion script

Create a file named `ingest_employees.py`:

```python
import os
import csv
import json
import requests

BASE_URL = "https://api.datalinks.com/api/v1"

token = os.environ["DATALINKS_TOKEN"]
namespace = "hr_demo"
dataset_name = "employee_records"

url = f"{BASE_URL}/ingest/{namespace}/{dataset_name}"

headers = {
    "Authorization": f"Bearer {token}",
    "Content-Type": "application/json",
}

# Read data from CSV file
data = []
with open("employee_records_sample.csv", "r") as f:
    reader = csv.DictReader(f)
    for row in reader:
        data.append(row)

payload = {"data": data}

response = requests.post(url, headers=headers, json=payload, timeout=60)

print("Status:", response.status_code)
print(json.dumps(response.json(), indent=2))
```

### Step 4: Run the script

```bash
python ingest_employees.py
```

A successful response returns status 200 with information about the ingested records.

### How this works

The request:

- Calls `POST /ingest/{namespace}/{datasetName}` to load data into an existing dataset
- Authenticates using a bearer token in the `Authorization` header
- Sends data as a JSON array of objects, where each object represents one record
- Returns information about successful and failed records

The `data` field accepts an array of dictionaries. Each dictionary represents a row, with keys as column names and values as the data for that column.

### Common issues

**401 Unauthorized**

- Confirm the token environment variable is set correctly
- Verify the `Authorization` header uses the `Bearer` prefix

**404 Not Found**

- Check that the namespace and dataset name match an existing dataset
- Verify you have permission to access the dataset

**400 Bad Request**

- Ensure the `data` field contains a valid JSON array
- Verify each record is a dictionary with string keys

## Cleaning data with inference steps

DataLinks can automatically transform and clean your data during ingestion using inference steps. These steps run in sequence to extract structure, normalize schemas, and validate data quality.

### Available inference steps

| Step         | Purpose                                         |
| ------------ | ----------------------------------------------- |
| `table`      | Extract structured data from unstructured text  |
| `rows`       | Extract rows from JSON content in a column      |
| `normalize`  | Standardize column names to a consistent schema |
| `validate`   | Verify data quality against expected patterns   |
| `reverseGeo` | Extract location data from coordinates          |

Use the `table` step to extract structured data from unstructured text:

```json
{
    "type": "table",
    "deriveFrom": "raw_text",
    "helperPrompt": "Extract employee name, department, and salary from each line."
}
```

The `deriveFrom` field specifies which column contains the text to parse. The `helperPrompt` guides the AI to improve extraction accuracy.

### Rows extraction

Use the `rows` step to extract rows from JSON content stored in a column:

```json
{
    "type": "rows",
    "deriveFrom": "json_data"
}
```

### Normalize

Use the `normalize` step to standardize column names and formats:

```json
{
    "type": "normalize",
    "mode": "all-in-one",
    "targetCols": {
        "employee_name": "Full name of the employee",
        "department": "Work department name",
        "salary": "Annual salary in USD as integer"
    },
    "helperPrompt": "This data contains HR employee records."
}
```

The normalize step supports three modes:

| Mode             | Use case                                                       |
| ---------------- | -------------------------------------------------------------- |
| `all-in-one`     | Normalize all columns in a single pass. Best for most cases.   |
| `field-by-field` | Normalize each field individually. Use for complex schemas.    |
| `embeddings`     | Use embeddings to match column names. Best for large datasets. |

### Validate

Use the `validate` step to verify data quality:

```json
{
    "type": "validate",
    "mode": "fields",
    "columns": ["email", "phone"]
}
```

Validation modes:

| Mode     | Behavior                                        |
| -------- | ----------------------------------------------- |
| `rows`   | Validates entire rows against expected patterns |
| `fields` | Validates specific columns                      |
| `regex`  | Validates using regular expressions             |

### ReverseGeo

Use the `reverseGeo` step to extract location data from coordinates:

```json
{
    "type": "reverseGeo",
    "deriveFrom": "coordinates"
}
```

## Uploading large files

For files larger than 5MB, use multipart upload for better reliability.

## Ingest data in the web platform

If you prefer a visual interface, you can ingest data directly in the DataLinks web platform. This approach is ideal for quick uploads or when you want to preview how DataLinks will process your data.

To ingest data in the web platform, follow these steps:

1. **Log in** to your DataLinks account.
2. Navigate to the dataset where you want to ingest data.
3. Click anywhere in the **Click to upload** area to open the file explorer.
4. Select your data file (JSON, CSV, or TXT), then click **Open**.
5. Wait while DataLinks ingests your data.
6. Click the **eye icon** under Ingestion Results to review the data and confirm that everything looks correct.
7. Scroll down to the bottom of the page, then click **Upload data**.
8. Click **Append** to confirm.

<Check>
  Your data will be added to the dataset and will be ready for linking and querying.
</Check>

For unstructured data, enter helper prompts and field definitions before uploading. Files attached before entering prompts will not have transformations applied.

## Next steps

Once your data is ingested, you can:

- **Review and manage connections** between datasets. See [How to Manage Connections](/how-to/manage-connections).
- **Query your data** using natural language or structured queries. See [How to Query Your Data](/how-to/query-data).
- **Rebuild links** if you have ingested new data that should be connected to existing datasets.