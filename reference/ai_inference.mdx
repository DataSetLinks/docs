---
title: "AI Inference"
sidebarTitle: "AI Inference"
---

## Overview

When you ingest data into DataLinks, you can include an inference pipeline that transforms your data before it is stored. The pipeline is a sequence of steps that extract structure from unstructured text, normalize column names to a target schema, validate data quality, and enrich records with geographic coordinates.

Inference steps are specified in the `infer.steps` array of an ingestion or preview request. Steps execute in order: the output of each step becomes the input for the next.

### Step types at a glance

| Step | `type` value | Uses LLM | Purpose |
|------|-------------|----------|---------|
| [Table extraction](#table-extraction) | `table` | Yes | Extract structured columns from unstructured text |
| [Rows](#rows) | `rows` | No | Expand a JSON array stored in a column into rows |
| [Normalize](#normalize) | `normalize` | Depends on mode | Map extracted column names to a target schema |
| [Validate](#validate) | `validate` | Yes | Check data quality and flag invalid rows |
| [ReverseGeo](#reversegeo) | `reverseGeo` | No | Add latitude/longitude from location names |

## Testing with Preview

Before committing data, use the Preview endpoint to run your inference pipeline without saving anything. This is the recommended way to iterate on your `helperPrompt`, step configuration, and schema until the output looks right.

```bash
curl -H "Authorization: Bearer YOUR_TOKEN" \
     -H "Content-Type: application/json" \
     -X POST https://api.datalinks.com/api/preview \
     --data '{
       "data": [
         {"raw": "Alice Chen, 34, Senior Engineer at Northwind, alice@northwind.io"}
       ],
       "infer": {
         "steps": [
           {
             "type": "table",
             "deriveFrom": "raw",
             "helperPrompt": "Each entry contains a name, age, job title, company, and email."
           }
         ]
       }
     }'
```

Once you are satisfied with the results, switch to the ingestion endpoint to persist the data:

```bash
curl -H "Authorization: Bearer YOUR_TOKEN" \
     -H "Content-Type: application/json" \
     -X POST https://api.datalinks.com/api/ingest/NAMESPACE/DATASET_NAME \
     --data '{ ... same body ... }'
```

<Tip>
Always start with Preview. Inference uses LLM calls that are easier to tune in a tight feedback loop before you commit to a full ingestion run.
</Tip>

## How the pipeline works

Steps execute sequentially in the order you list them. A typical pipeline follows this pattern:

```
Raw data
  → Table extraction (unstructured text → columns)
    → Normalize (inconsistent column names → target schema)
      → Validate (flag rows that fail quality checks)
        → Stored in DataLinks
```

You can use any combination of steps. A single step is fine. So is skipping straight to normalize if your data is already tabular but has inconsistent column names.

The full request structure looks like this:

```json
{
  "data": [ ... ],
  "infer": {
    "steps": [
      { "type": "table", "deriveFrom": "...", "helperPrompt": "..." },
      { "type": "normalize", "mode": "...", "targetCols": { ... } },
      { "type": "validate", "mode": "...", "columns": [ ... ] }
    ]
  }
}
```